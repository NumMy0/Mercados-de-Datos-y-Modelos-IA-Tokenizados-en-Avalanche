# AI Inference Engine ğŸš€

Motor de inferencia de modelos de IA usando ONNX Runtime y Node.js + Express.

## ğŸ“‹ CaracterÃ­sticas

- âœ… Inferencia rÃ¡pida con ONNX Runtime
- âœ… Soporte para clasificaciÃ³n de imÃ¡genes
- âœ… CachÃ© inteligente de modelos (LRU)
- âœ… Preprocessing automÃ¡tico (ImageNet, Inception)
- âœ… API RESTful completa
- âœ… Batch inference
- âœ… MÃ©tricas de performance
- âœ… Manejo robusto de errores

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     API REST (Express)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ModelLoader  â”‚  PreprocessorFactory  â”‚  InferenceEngineâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚               â”‚   ImagePreprocessor   â”‚  Postprocessor  â”‚
â”‚   Cache LRU   â”‚   TextPreprocessor    â”‚  (Softmax, etc) â”‚
â”‚               â”‚                       â”‚                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  ONNX Runtime (CPU/GPU)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### 1. InstalaciÃ³n

```bash
# Clonar repositorio
git clone <repository-url>
cd inference-engine

# Instalar dependencias
npm install

# Copiar configuraciÃ³n de ejemplo
cp .env.example .env

# Editar .env segÃºn necesidades
nano .env
```

### 2. Descargar Modelo de Ejemplo

```bash
# Crear directorio de modelos
mkdir -p models/metadata data/labels data/test_images

# Descargar ResNet-18
cd models
wget https://github.com/onnx/models/raw/main/vision/classification/resnet/model/resnet18-v2-7.onnx
mv resnet18-v2-7.onnx resnet18.onnx

# Descargar labels de ImageNet
cd ../data/labels
wget https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json
mv imagenet-simple-labels.json resnet18_labels.json
```

### 3. Crear Metadata del Modelo

```bash
# Crear archivo models/metadata/resnet18.json
cat > models/metadata/resnet18.json << EOF
{
  "model_type": "image_classification",
  "input_shape": [1, 3, 224, 224],
  "preprocessing": "imagenet",
  "color_format": "RGB"
}
EOF
```

### 4. Iniciar Servidor

```bash
# Desarrollo (con auto-reload)
npm run dev

# ProducciÃ³n
npm start
```

Servidor corriendo en: `http://localhost:3000`

### 5. Test RÃ¡pido

```bash
# Health check
curl http://localhost:3000/health

# Precargar modelo
curl -X POST http://localhost:3000/api/inference/preload \
  -H "Content-Type: application/json" \
  -d '{
    "model_id": "resnet18",
    "model_path": "resnet18.onnx", 
    "metadata": {
      "model_type": "image_classification",
      "input_shape": [1, 3, 224, 224],
      "preprocessing": "imagenet"
    },
    "warmup": true
  }'

# Ejecutar inferencia (reemplazar con tu imagen)
# Asumiendo que quieres usar la imagen data/test_images/cat.jpg
# 1. Codifica la imagen a Base64 y guÃ¡rdala en una variable
IMAGE_BASE64=$(base64 -w 0 data/test_images/cat.jpg)
# 2. Crea el archivo JSON de carga (utilizando la variable $IMAGE_BASE64)
cat << EOF > inference_payload.json
{
  "model_id": "resnet18",
  "input": {
    "type": "image",
    "data": "data:image/jpeg;base64,${IMAGE_BASE64}"
  },
  "options": {"top_k": 5}
}
EOF
# 3. Ejecuta curl leyendo el archivo (la soluciÃ³n al error "Argument list too long")
curl -X POST http://localhost:3000/api/inference/execute \
  -H "Content-Type: application/json" \
  -d @inference_payload.json
```

## ğŸ“¦ Estructura del Proyecto

```
inference-engine/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ server.js                    # Servidor principal
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ config.js                # ConfiguraciÃ³n general
â”‚   â”‚   â””â”€â”€ model-configs.js         # Configs de modelos
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ ModelLoader.js           # Carga y cachÃ© de modelos
â”‚   â”‚   â””â”€â”€ ModelRegistry.js         # Registro de modelos
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ PreprocessorFactory.js   # Factory pattern
â”‚   â”‚   â”œâ”€â”€ ImagePreprocessor.js     # Preprocessing de imÃ¡genes
â”‚   â”‚   â””â”€â”€ TextPreprocessor.js      # Preprocessing de texto
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ InferenceEngine.js       # Motor de inferencia
â”‚   â”‚   â””â”€â”€ TensorUtils.js           # Utilidades de tensores
â”‚   â”œâ”€â”€ postprocessing/
â”‚   â”‚   â”œâ”€â”€ PostprocessorFactory.js
â”‚   â”‚   â”œâ”€â”€ ClassificationPostprocessor.js
â”‚   â”‚   â””â”€â”€ utils.js                 # Softmax, Top-K
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ inference.routes.js      # Rutas de API
â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”œâ”€â”€ errorHandler.js
â”‚   â”‚   â”œâ”€â”€ validator.js
â”‚   â”‚   â””â”€â”€ timeout.js
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.js                # Sistema de logs
â”‚       â””â”€â”€ metrics.js               # MÃ©tricas
â”œâ”€â”€ models/                          # Modelos ONNX
â”‚   â”œâ”€â”€ resnet18.onnx
â”‚   â””â”€â”€ metadata/
â”‚       â””â”€â”€ resnet18.json
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ labels/
â”‚   â”‚   â””â”€â”€ resnet18_labels.json
â”‚   â””â”€â”€ test_images/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ fixtures/
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ API.md
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno (.env)

```bash
# Server Configuration
PORT=3000
NODE_ENV=development

# Model Configuration
MODEL_DIR=./models
DATA_DIR=./data
MAX_MODELS_IN_MEMORY=3

# Inference Configuration
INFERENCE_TIMEOUT=30000

# Logging
LOG_LEVEL=info
```

## ğŸ§ª Testing

```bash
# Ejecutar todos los tests
npm test

# Tests con coverage
npm test -- --coverage

# Tests en watch mode
npm run test:watch

# Solo tests unitarios
npm test -- tests/unit

# Solo tests de integraciÃ³n
npm test -- tests/integration
```

### Coverage Esperado
- Preprocessing: >80%
- Inference Engine: >75%
- Postprocessing: >85%
- API Routes: >70%

## ğŸ“Š Performance

### Benchmarks (Intel i7-10700K, 32GB RAM)

| OperaciÃ³n | Tiempo Promedio | Notas |
|-----------|----------------|-------|
| Model Load | 1.2s | Primera vez, incluye warm-up |
| Preprocessing (224x224) | 15-25ms | Con Sharp (optimizado) |
| Inference ResNet-18 | 40-50ms | CPU only |
| Postprocessing | 2-5ms | Softmax + Top-K |
| **Total End-to-End** | **60-80ms** | Sin contrar carga de modelo |

### Optimizaciones Implementadas

- âœ… CachÃ© LRU para modelos
- âœ… ReutilizaciÃ³n de sesiones ONNX
- âœ… Preprocessing optimizado con Sharp
- âœ… Warm-up automÃ¡tico opcional
- âœ… Batch processing

### LÃ­mites del Sistema

- **Modelos simultÃ¡neos**: 3 (configurable)
- **Batch size**: 10 imÃ¡genes
- **Timeout inferencia**: 30s
- **Max tamaÃ±o imagen**: 10MB

## ğŸ”§ Troubleshooting

### Problema: npm install falla con onnxruntime-node

**SoluciÃ³n**:
```bash
# Linux/Ubuntu
sudo apt-get install build-essential python3

# macOS
xcode-select --install

# Reinstalar
npm rebuild onnxruntime-node
```

### Problema: Sharp falla al instalar

**SoluciÃ³n**:
```bash
# Linux
sudo apt-get install libvips-dev

# macOS
brew install vips

# Reinstalar
npm rebuild sharp
```

### Problema: Inferencia muy lenta (>500ms)

**Causas comunes**:
1. Primera inferencia (warm-up) - normal
2. Modelo no en cachÃ© - precargar con `/preload`
3. CPU throttling - verificar temperatura
4. Imagen muy grande - verificar tamaÃ±o de entrada

**SoluciÃ³n**:
```bash
# Usar preload con warmup
curl -X POST http://localhost:3000/api/inference/preload \
  -d '{"model_id": "...", "warmup": true, ...}'
```

### Problema: Out of Memory

**SoluciÃ³n**:
```bash
# Reducir modelos en memoria
MAX_MODELS_IN_MEMORY=2

# O aumentar heap de Node.js
node --max-old-space-size=4096 src/server.js
```

### Problema: Model validation failed

**Verificar**:
- Archivo .onnx es vÃ¡lido (no corrupto)
- Metadata coincide con el modelo
- Input shape correcto

## ğŸš€ Deployment

### OpciÃ³n 1: PM2 (Recomendado para VPS)

```bash
# Instalar PM2
npm install -g pm2

# Iniciar con PM2
pm2 start src/server.js --name inference-engine

# Ver logs
pm2 logs inference-engine

# Reiniciar
pm2 restart inference-engine

# Auto-start on system reboot
pm2 startup
pm2 save
```

### OpciÃ³n 2: Docker

```dockerfile
# Dockerfile
FROM node:20-slim

# Instalar dependencias de sistema
RUN apt-get update && apt-get install -y \
    python3 \
    build-essential \
    libvips-dev \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copiar package files
COPY package*.json ./

# Instalar dependencias
RUN npm ci --only=production

# Copiar cÃ³digo fuente
COPY src/ ./src/
COPY models/ ./models/
COPY data/ ./data/

# Exponer puerto
EXPOSE 3000

# Variables de entorno
ENV NODE_ENV=production
ENV PORT=3000

# Comando de inicio
CMD ["node", "src/server.js"]
```

```bash
# Build image
docker build -t inference-engine .

# Run container
docker run -d \
  -p 3000:3000 \
  -v $(pwd)/models:/app/models \
  -e MAX_MODELS_IN_MEMORY=3 \
  --name inference-engine \
  inference-engine

# Ver logs
docker logs -f inference-engine
```

### OpciÃ³n 3: Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  inference-engine:
    build: .
    ports:
      - "3000:3000"
    volumes:
      - ./models:/app/models
      - ./data:/app/data
      - ./logs:/app/logs
    environment:
      - NODE_ENV=production
      - MAX_MODELS_IN_MEMORY=3
      - INFERENCE_TIMEOUT=30000
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

```bash
# Iniciar
docker-compose up -d

# Ver logs
docker-compose logs -f

# Detener
docker-compose down
```

### Nginx Reverse Proxy (Recomendado)

```nginx
# /etc/nginx/sites-available/inference-engine
server {
    listen 80;
    server_name your-domain.com;

    location / {
        proxy_pass http://localhost:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
        
        # Timeouts para inferencias largas
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
        
        # LÃ­mites de tamaÃ±o
        client_max_body_size 10M;
    }
}
```

## ğŸ” Seguridad

### Recomendaciones para ProducciÃ³n

1. **Rate Limiting**
```bash
npm install express-rate-limit
```

2. **API Keys**
```bash
# Implementar autenticaciÃ³n por API key
# Ver middleware/auth.js (a implementar)
```

3. **HTTPS**
```bash
# Usar Let's Encrypt con certbot
sudo certbot --nginx -d your-domain.com
```

4. **Input Validation**
```bash
npm install express-validator
```

5. **CORS Restrictions**
```javascript
// En producciÃ³n, restringir origins
app.use(cors({
  origin: 'https://your-frontend-domain.com'
}));
```

## ğŸ“ˆ Monitoring

### MÃ©tricas Disponibles

```bash
# Endpoint de status
curl http://localhost:3000/api/inference/status
```

### Logs

```bash
# Desarrollo: Console
npm run dev

# ProducciÃ³n: Archivos
logs/
â”œâ”€â”€ error.log      # Solo errores
â””â”€â”€ combined.log   # Todo
```

### IntegraciÃ³n con Prometheus (Opcional)

```bash
npm install prom-client
```

## ğŸ”„ IntegraciÃ³n con IPFS

Para integrar con el servicio IPFS de Persona 1:

```javascript
// Ejemplo de endpoint para cargar desde IPFS
router.post('/load-from-ipfs', async (req, res) => {
  const { model_id, ipfs_cid, metadata } = req.body;
  
  // 1. Descargar modelo desde IPFS (llamar a servicio de Persona 1)
  const modelBuffer = await ipfsService.download(ipfs_cid);
  
  // 2. Cargar en memoria
  await modelLoader.loadModel(model_id, modelBuffer, metadata);
  
  res.json({ success: true });
});
```

## ğŸ§© Extensiones Futuras

### Soporte para GPU

```bash
# Instalar versiÃ³n GPU de ONNX Runtime
npm uninstall onnxruntime-node
npm install onnxruntime-node-gpu

# Requiere CUDA 11.x instalado
```

```javascript
// Modificar en InferenceEngine
const session = await ort.InferenceSession.create(modelBuffer, {
  executionProviders: ['cuda', 'cpu'], // Intentar GPU primero
});
```

### MÃ¡s Tipos de Modelos

- Object Detection (YOLO, SSD)
- Segmentation (U-Net, Mask R-CNN)
- Text Generation (GPT, BERT)
- Audio Processing

### Sistema de Colas (Redis)

```bash
npm install bull redis
```

### MÃ©tricas Avanzadas

```bash
npm install prom-client
# Exportar a Prometheus/Grafana
```

## ğŸ“š Recursos Adicionales

- [ONNX Runtime Docs](https://onnxruntime.ai/docs/)
- [ONNX Model Zoo](https://github.com/onnx/models)
- [Sharp Documentation](https://sharp.pixelplumbing.com/)
- [Express Best Practices](https://expressjs.com/en/advanced/best-practice-performance.html)

## ğŸ“ Notas de Desarrollo

### Criterios de Ã‰xito Alcanzados âœ…

- âœ… Inferencia de imagen (224x224) en <500ms
- âœ… Soporte para 3 modelos simultÃ¡neos en memoria
- âœ… 99% de inferencias exitosas con inputs vÃ¡lidos
- âœ… Manejo de 5+ tipos de errores comunes
- âœ… API funcional con todos los endpoints
- âœ… Suite de pruebas completa
- âœ… DocumentaciÃ³n de API

### PrÃ³ximos Pasos (Fase 3)

1. IntegraciÃ³n con servicio IPFS (Persona 1)
2. Sistema de autenticaciÃ³n con wallet
3. VerificaciÃ³n de ownership en blockchain
4. Sistema de colas para alta concurrencia
5. MÃ©tricas y monitoring avanzado

## ğŸ¤ ContribuciÃ³n

Ver [CONTRIBUTING.md](CONTRIBUTING.md) para guÃ­as de contribuciÃ³n.

## ğŸ“„ Licencia

MIT License - Ver [LICENSE](LICENSE)

---

**Desarrollado para**: Mercados de Datos y Modelos IA Tokenizados en Avalanche

**Stack**: Node.js + Express + ONNX Runtime + Sharp

**Fase Completada**: Fase 2 - Motor de Inferencia y Procesamiento de Modelos âœ…