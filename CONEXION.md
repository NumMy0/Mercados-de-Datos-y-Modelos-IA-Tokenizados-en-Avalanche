# ğŸš€ GuÃ­a de ConexiÃ³n Frontend - Inference Engine

Esta guÃ­a explica cÃ³mo ejecutar ambos servicios (frontend y inference-engine) de manera conjunta.

## ğŸ“‹ Prerequisitos

- Node.js >= 18.0.0
- npm o pnpm
- MetaMask instalado en el navegador

## ğŸ”§ InstalaciÃ³n

### OpciÃ³n 1: InstalaciÃ³n AutomÃ¡tica (Recomendado)

Desde el directorio raÃ­z del proyecto:

```bash
npm install
npm run install:all
```

### OpciÃ³n 2: InstalaciÃ³n Manual

```bash
# Instalar dependencias del root (concurrently)
npm install

# Instalar dependencias del frontend
cd frontend
npm install

# Instalar dependencias del inference-engine
cd ../inference-engine
npm install
```

## âš™ï¸ ConfiguraciÃ³n

### Frontend (.env)

El archivo `frontend/.env` ya estÃ¡ configurado con:

```env
# Inference Engine API
VITE_INFERENCE_API_URL=http://localhost:3001/api
```

### Inference Engine (.env)

El archivo `inference-engine/.env` ya estÃ¡ configurado con:

```env
PORT=3001
HOST=0.0.0.0
CORS_ORIGINS=http://localhost:5173,http://localhost:3000,http://127.0.0.1:5173,http://127.0.0.1:3000
```

## ğŸš€ EjecuciÃ³n

### OpciÃ³n 1: Ejecutar Ambos Servicios Juntos (Recomendado)

Desde el directorio raÃ­z:

```bash
npm run dev
```

Esto iniciarÃ¡:
- âœ… Frontend en `http://localhost:5173`
- âœ… Inference Engine en `http://localhost:3001`

### OpciÃ³n 2: Ejecutar Servicios por Separado

#### Terminal 1 - Frontend:
```bash
cd frontend
npm run dev
```

#### Terminal 2 - Inference Engine:
```bash
cd inference-engine
npm run dev
```

## ğŸ§ª Verificar la ConexiÃ³n

1. **Health Check del Inference Engine:**
   ```bash
   curl http://localhost:3001/health
   ```
   
   DeberÃ­as recibir:
   ```json
   {
     "status": "healthy",
     "memory": {...},
     "uptime": "..."
   }
   ```

2. **Desde el Frontend:**
   - Abre `http://localhost:5173`
   - Navega a un modelo
   - Haz clic en "Ver Detalles"
   - Haz clic en "Ejecutar Inferencia con este Modelo"
   - Selecciona una imagen o texto
   - Haz clic en "Ejecutar Inferencia"

## ğŸ“¡ Endpoints del Inference Engine

### Health Check
- **GET** `http://localhost:3001/health`
- Verifica que el servicio estÃ© funcionando

### Ejecutar Inferencia
- **POST** `http://localhost:3001/api/inference`
- Body:
  ```json
  {
    "modelId": "1",
    "ipfsHash": "QmXxx...",
    "input": {
      "type": "image",
      "data": "data:image/jpeg;base64,..."
    }
  }
  ```

### Cargar Modelo desde IPFS
- **POST** `http://localhost:3001/api/models/load`
- Body:
  ```json
  {
    "ipfsHash": "QmXxx..."
  }
  ```

### Obtener Metadata de Modelo
- **GET** `http://localhost:3001/api/models/metadata/{ipfsHash}`

## ğŸ¯ Flujo de Inferencia Completo

```
1. Usuario â†’ Frontend â†’ Selecciona modelo
                â†“
2. Usuario â†’ Frontend â†’ Abre "Ejecutar Inferencia"
                â†“
3. Usuario â†’ Frontend â†’ Sube imagen/texto
                â†“
4. Frontend â†’ POST /api/inference â†’ Inference Engine
                â†“
5. Inference Engine â†’ Carga modelo desde IPFS (si es necesario)
                â†“
6. Inference Engine â†’ Preprocesa entrada
                â†“
7. Inference Engine â†’ Ejecuta inferencia ONNX
                â†“
8. Inference Engine â†’ Posprocesa resultados
                â†“
9. Inference Engine â†’ JSON Response â†’ Frontend
                â†“
10. Frontend â†’ Muestra resultados al usuario
```

## ğŸ” Debugging

### Verificar CORS

Si ves errores de CORS en la consola del navegador:

1. Verifica que el Inference Engine estÃ© corriendo en el puerto 3001
2. Verifica los orÃ­genes permitidos en `inference-engine/.env`:
   ```env
   CORS_ORIGINS=http://localhost:5173,http://localhost:3000
   ```

### Verificar ConexiÃ³n

Si el frontend no puede conectar con el inference engine:

1. Verifica que ambos servicios estÃ©n corriendo
2. Verifica la URL en `frontend/.env`:
   ```env
   VITE_INFERENCE_API_URL=http://localhost:3001/api
   ```

### Ver Logs

Los logs del inference engine aparecerÃ¡n en la terminal donde lo ejecutaste.
Busca mensajes como:

```
ğŸš€ Inference Engine started on http://0.0.0.0:3001
âœ… CORS enabled for: http://localhost:5173, http://localhost:3000
ğŸ“Š Environment: development
```

## ğŸ“¦ Estructura de Archivos Clave

```
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ InferenceModal.vue       # Modal de inferencia
â”‚   â”‚   â”‚   â””â”€â”€ ModelDetailsModal.vue    # Integra InferenceModal
â”‚   â”‚   â””â”€â”€ composables/
â”‚   â”‚       â””â”€â”€ useInference.ts          # LÃ³gica de conexiÃ³n con API
â”‚   â””â”€â”€ .env                             # ConfiguraciÃ³n frontend
â”‚
â”œâ”€â”€ inference-engine/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ server.js                    # Servidor Express
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”‚   â””â”€â”€ config.js                # ConfiguraciÃ³n CORS
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ inference.routes.js      # Rutas de inferencia
â”‚   â”‚       â””â”€â”€ model.routes.js          # Rutas de modelos
â”‚   â””â”€â”€ .env                             # ConfiguraciÃ³n inference engine
â”‚
â””â”€â”€ package.json                         # Scripts para ejecutar ambos
```

## ğŸ› ï¸ SoluciÃ³n de Problemas Comunes

### Error: "Cannot connect to inference engine"

- **Causa:** El inference engine no estÃ¡ corriendo
- **SoluciÃ³n:** Ejecuta `npm run dev` desde el directorio raÃ­z

### Error: "CORS policy blocked"

- **Causa:** OrÃ­genes no configurados correctamente
- **SoluciÃ³n:** Verifica `CORS_ORIGINS` en `inference-engine/.env`

### Error: "Model not found"

- **Causa:** El modelo no se pudo cargar desde IPFS
- **SoluciÃ³n:** Verifica que el `ipfsHash` sea correcto

### Error: "Timeout"

- **Causa:** La inferencia tardÃ³ demasiado
- **SoluciÃ³n:** Aumenta `INFERENCE_TIMEOUT` en `inference-engine/.env`

## ğŸ“š Recursos Adicionales

- [DocumentaciÃ³n de la API de Inferencia](./inference-engine/docs/API.md)
- [ConfiguraciÃ³n de Modelos](./inference-engine/src/config/model-configs.js)
- [Ejemplos de Uso](./inference-engine/tests/fixtures/sample_inputs.js)

## âœ¨ Funcionalidades Implementadas

- âœ… ConexiÃ³n Frontend â†” Inference Engine
- âœ… CORS configurado correctamente
- âœ… Modal de inferencia interactivo
- âœ… Soporte para imÃ¡genes y texto
- âœ… ValidaciÃ³n de licencias (preparado)
- âœ… Carga de modelos desde IPFS
- âœ… Procesamiento en tiempo real
- âœ… Manejo de errores
- âœ… Logs detallados

## ğŸ‰ Â¡Listo!

Ahora tienes un marketplace de IA completamente funcional con capacidades de inferencia en tiempo real.
