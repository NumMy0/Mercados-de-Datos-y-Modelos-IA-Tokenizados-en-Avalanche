# ğŸ§  Motor de Inferencia - DocumentaciÃ³n Detallada

## Arquitectura Interna

```
Inference Engine (Node.js + Express)
â”‚
â”œâ”€â”€ ğŸ“¦ Model Loader
â”‚   â”œâ”€â”€ IPFS Service (descarga modelos)
â”‚   â”œâ”€â”€ Metadata Parser (valida formato)
â”‚   â”œâ”€â”€ LRU Cache (3 modelos en memoria)
â”‚   â””â”€â”€ Model Registry (Ã­ndice de modelos)
â”‚
â”œâ”€â”€ ğŸ”„ Preprocessing Pipeline
â”‚   â”œâ”€â”€ Image Preprocessor (Sharp)
â”‚   â”‚   â”œâ”€â”€ Resize (224x224, 299x299)
â”‚   â”‚   â”œâ”€â”€ Normalize (ImageNet, Inception)
â”‚   â”‚   â””â”€â”€ Color conversion (RGB/BGR)
â”‚   â””â”€â”€ Preprocessor Factory (extensible)
â”‚
â”œâ”€â”€ âš¡ Inference Engine
â”‚   â”œâ”€â”€ ONNX Runtime (CPU/GPU)
â”‚   â”œâ”€â”€ Session Manager
â”‚   â””â”€â”€ Warmup System
â”‚
â””â”€â”€ ğŸ“Š Postprocessing Pipeline
    â”œâ”€â”€ Classification Postprocessor
    â”‚   â”œâ”€â”€ Softmax
    â”‚   â”œâ”€â”€ Top-K predictions
    â”‚   â””â”€â”€ Confidence scores
    â””â”€â”€ Postprocessor Factory (extensible)
```

## CachÃ© LRU de Modelos

El motor implementa un sistema de cachÃ© LRU (Least Recently Used) que mantiene hasta **3 modelos** en memoria:

```javascript
// Ejemplo de funcionamiento del cachÃ©
// Estado inicial: []
loadModel("resnet18")     â†’ Cache: [resnet18]
loadModel("mobilenet")    â†’ Cache: [resnet18, mobilenet]
loadModel("efficientnet") â†’ Cache: [resnet18, mobilenet, efficientnet]
loadModel("vgg16")        â†’ Cache: [mobilenet, efficientnet, vgg16]
//                          â†‘ resnet18 fue removido (LRU)
```

**Beneficios**:
- âœ… Inferencias subsecuentes ~60-80ms (sin carga de modelo)
- âœ… Primera inferencia incluye warm-up automÃ¡tico
- âœ… Uso eficiente de memoria RAM

## Pipeline Completo de Procesamiento

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. RECEPCIÃ“N                                            â”‚
â”‚    - Request HTTP POST /api/inference/execute           â”‚
â”‚    - ValidaciÃ³n de payload                              â”‚
â”‚    - ExtracciÃ³n de model_id e input_data                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. VERIFICACIÃ“N DE LICENCIA                             â”‚
â”‚    - Query a Smart Contract: hasValidLicense()          â”‚
â”‚    - Si no tiene licencia â†’ Error 403                   â”‚
â”‚    - Si tiene licencia â†’ Continuar                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. CARGA DE MODELO                                      â”‚
â”‚    - Verificar cachÃ© LRU                                â”‚
â”‚    - Si estÃ¡ en cachÃ© â†’ Usar sesiÃ³n ONNX existente      â”‚
â”‚    - Si no estÃ¡ en cachÃ©:                               â”‚
â”‚      â””â”€â†’ Descargar desde IPFS (usando metadata CID)     â”‚
â”‚      â””â”€â†’ Parsear metadata.json                          â”‚
â”‚      â””â”€â†’ Cargar en ONNX Runtime                         â”‚
â”‚      â””â”€â†’ Guardar en cachÃ© (remover LRU si estÃ¡ lleno)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. PREPROCESSING                                        â”‚
â”‚    - Detectar tipo de input (base64_jpeg, url, etc)     â”‚
â”‚    - Decodificar imagen con Sharp                       â”‚
â”‚    - Aplicar pipeline segÃºn metadata:                   â”‚
â”‚      â€¢ ImageNet: resize(224,224) + normalize([0.485,    â”‚
â”‚        0.456, 0.406], [0.229, 0.224, 0.225])            â”‚
â”‚      â€¢ Inception: resize(299,299) + normalize([0.5,     â”‚
â”‚        0.5, 0.5], [0.5, 0.5, 0.5])                      â”‚
â”‚    - Convertir a tensor Float32Array                    â”‚
â”‚    - Reshape a [1, 3, H, W] (NCHW format)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. INFERENCIA (ONNX Runtime)                            â”‚
â”‚    - Ejecutar session.run(tensor)                       â”‚
â”‚    - Output: Float32Array con logits [1, num_classes]   â”‚
â”‚    - Tiempo tÃ­pico: 40-50ms (ResNet-18 en CPU)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. POSTPROCESSING                                       â”‚
â”‚    - Aplicar softmax a logits â†’ probabilidades          â”‚
â”‚    - Ordenar por confianza (desc)                       â”‚
â”‚    - Tomar Top-K (ej. top_k=5)                          â”‚
â”‚    - Mapear Ã­ndices a labels usando labels.json         â”‚
â”‚    - Calcular mÃ©tricas (confidence, entropy)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. RESPUESTA                                            â”‚
â”‚    {                                                    â”‚
â”‚      "predictions": [...],                              â”‚
â”‚      "metrics": {...},                                  â”‚
â”‚      "metadata": {...}                                  â”‚
â”‚    }                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## API Endpoints

### POST `/api/inference/execute`

Ejecuta una inferencia con un modelo especÃ­fico.

**Request Body**:
```json
{
  "model_id": "resnet18_cache",
  "execution_mode": "sync",
  "input_data": {
    "format": "base64_jpeg",
    "content": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAA..."
  },
  "options": {
    "top_k": 5,
    "threshold": 0.1,
    "timeout": 30000
  }
}
```

**Response** (Success 200):
```json
{
  "success": true,
  "model_id": "resnet18_cache",
  "predictions": [
    {
      "class": "golden_retriever",
      "confidence": 0.9234,
      "class_id": 207
    },
    {
      "class": "labrador_retriever",
      "confidence": 0.0543,
      "class_id": 208
    }
  ],
  "metrics": {
    "inference_time_ms": 45,
    "preprocessing_time_ms": 18,
    "postprocessing_time_ms": 3,
    "total_time_ms": 66
  }
}
```

**Errores Comunes**:
```json
// 403 - Sin licencia vÃ¡lida
{
  "error": "No valid license",
  "message": "You need an active license to use this model"
}

// 404 - Modelo no encontrado
{
  "error": "Model not found",
  "suggestion": "Use /api/models/load endpoint first"
}
```

### POST `/api/models/load`

Carga un modelo desde IPFS y lo guarda en cachÃ©.

**Request Body**:
```json
{
  "tokenId": "1",
  "metadataCid": "QmXxx...abc",
  "warmup": true
}
```

**Response** (Success 200):
```json
{
  "success": true,
  "model_id": "resnet18_cache",
  "message": "Modelo cargado y en cachÃ©",
  "load_time_ms": 1243,
  "cacheStatus": {
    "current_size": 2,
    "max_size": 3,
    "models_in_cache": ["mobilenet_cache", "resnet18_cache"]
  }
}
```

### GET `/api/inference/formats`

Obtiene los formatos de entrada soportados.

### GET `/health`

Health check del servicio.

## Escenarios de Inferencia

### Escenario 1: Primera Inferencia (Cold Start)

```
Usuario â†’ Frontend â†’ Inference Engine
                           â”‚
                           â”œâ”€â†’ Verificar licencia (Smart Contract) âœ…
                           â”œâ”€â†’ Buscar en cachÃ© âŒ NOT FOUND
                           â”œâ”€â†’ Descargar desde IPFS â± ~1200ms
                           â”œâ”€â†’ Cargar en ONNX Runtime â± ~300ms
                           â”œâ”€â†’ Preprocessing â± ~18ms
                           â”œâ”€â†’ Inferencia â± ~45ms
                           â”œâ”€â†’ Postprocessing â± ~3ms
                           â””â”€â†’ Respuesta
                                   Total: ~1566ms
```

### Escenario 2: Inferencias Subsecuentes (Warm State)

```
Usuario â†’ Frontend â†’ Inference Engine
                           â”‚
                           â”œâ”€â†’ Verificar licencia âœ…
                           â”œâ”€â†’ Buscar en cachÃ© âœ… FOUND!
                           â”œâ”€â†’ Preprocessing â± ~18ms
                           â”œâ”€â†’ Inferencia (sesiÃ³n cacheada) â± ~45ms
                           â”œâ”€â†’ Postprocessing â± ~3ms
                           â””â”€â†’ Respuesta
                                   Total: ~66ms âš¡
```

## Performance Benchmarks

| OperaciÃ³n | Tiempo | Notas |
|-----------|--------|-------|
| Carga desde IPFS | ~1200ms | Primera vez |
| Warm-up ONNX | ~300ms | Incluido en carga |
| Hit de cachÃ© | ~0ms | InstantÃ¡neo |
| Preprocessing (224x224) | 15-25ms | Sharp optimizado |
| Inferencia ResNet-18 | 40-50ms | CPU Intel i7 |
| Postprocessing | 2-5ms | Softmax + Top-K |
| **Total (cold)** | **~1566ms** | Con descarga IPFS |
| **Total (warm)** | **~66ms** | Modelo en cachÃ© âš¡ |

## ConfiguraciÃ³n

### Variables de Entorno

```bash
# .env
PORT=3000
NODE_ENV=development
MODEL_DIR=./models
DATA_DIR=./data
MAX_MODELS_IN_MEMORY=3
INFERENCE_TIMEOUT=30000
LOG_LEVEL=info

# Blockchain
AVALANCHE_RPC_URL=https://api.avax-test.network/ext/bc/C/rpc
CONTRACT_ADDRESS=0x...
```

## Extensibilidad

### Agregar Nuevo Tipo de Modelo

1. Crear Preprocessor:
```javascript
// src/preprocessing/ObjectDetectionPreprocessor.js
class ObjectDetectionPreprocessor {
  async preprocess(input, metadata) {
    // LÃ³gica especÃ­fica
  }
}
```

2. Crear Postprocessor:
```javascript
// src/postprocessing/ObjectDetectionPostprocessor.js
class ObjectDetectionPostprocessor {
  process(output, options) {
    // LÃ³gica de bounding boxes
  }
}
```

3. Registrar en Factory:
```javascript
// src/preprocessing/PreprocessorFactory.js
if (metadata.model_type === 'object_detection') {
  return new ObjectDetectionPreprocessor();
}
```
